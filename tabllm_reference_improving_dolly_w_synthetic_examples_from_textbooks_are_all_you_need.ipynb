{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vg-55/btc_dataset/blob/main/tabllm_reference_improving_dolly_w_synthetic_examples_from_textbooks_are_all_you_need.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating synthetic examples for training an LLM, inspired from from \"Textbooks Are All You Need\" üöÄüë®‚Äçüíª\n",
        "\n",
        "In this notebook, we will leverage [Gretel's Tabular LLM](https://gretel.ai/tabular-llm) to generate new diverse, high-quality training examples building on these techniques to create better LLMs. Our goal with this notebook is to demonstrate how to get started creating high quality synthetic data for LLM training, and facilitate further research into safeguards for completion models.\n",
        "\n",
        "## Background\n",
        "Recent research has shown that training small, efficient language models (LLMs) on high-quality, diverse data can achieve state-of-the-art results, as demonstrated by models like Microsoft's \"phi-1.5\" (from their paper \"[Textbooks Are All You Need](https://arxiv.org/pdf/2306.11644.pdf)\"), [Orca2](https://huggingface.co/microsoft/Orca-2-7b), and [IBM's Granite](https://www.ibm.com/blog/watsonx-tailored-generative-ai/). Using similar techniques, we'll demonstate ways to inject randomness into the prompt in a way that gives rise to the generation of diverse datasets.\n",
        "\n",
        "Creating diverse training data is challenging, but vital to reduce overfitting and improve generalization. Techniques like including random word subsets in prompts, as done in [TinyStories](https://arxiv.org/abs/2305.07759), will be used.\n",
        "\n",
        "Compared to models trained on web data, ‚Äú[Textbooks Are All You Need II](https://arxiv.org/abs/2309.05463)‚Äù highlights additional advantages from using textbook-like data: \"the model seems to store and access the knowledge more efficiently\" and it has an \"attenuating effect on toxic content generation.\" However, as the authors note, \"although phi-1.5 has a lower propensity for generating toxic content...it is not immune.\" They posit phi-1.5's reliance on synthetic data \"provide[s] a useful platform for exploring these challenges further.\"\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "Before diving into the notebook, there are a couple of prerequisites:\n",
        "\n",
        "1. **Gretel API Key**: You'll need an API key from Gretel. If you don't have one already, you can obtain it from [Gretel's console](https://console.gretel.ai). This key will enable us to use Gretel's services for generating our synthetic datasets.\n",
        "\n",
        "2. **Access to Gretel's Tabular LLM**: To utilize the specific features of the Tabular LLM, you need to have access to the early preview. If you're not already signed up, you can request early access at [Gretel's Tabular LLM page](https://gretel.ai/tabular-llm).\n",
        "\n",
        "3. **Domain-specific training data**: To try this approach with your own data, you'll need a LLM training dataset in a standard input / output format, like you might load from HuggingFace or use to train your model. Or, get started quickly with the example below using the [databricks/dolly-15k](https://huggingface.co/datasets/databricks/databricks-dolly-15k) dataset.\n",
        "\n",
        "\n",
        "Let's get started!\n"
      ],
      "metadata": {
        "id": "0Ww5pmPF7tQW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSHf6P8i0ZIE"
      },
      "outputs": [],
      "source": [
        "!pip install -qq gretel-client datasets nltk #sklearn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "import yaml\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import pathlib\n",
        "from datasets import load_dataset\n",
        "from IPython.display import HTML\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from gretel_client import configure_session, create_or_get_unique_project\n",
        "from gretel_client.helpers import poll\n",
        "\n",
        "# Constants and Configurations\n",
        "PROJECT_NAME = \"synthetic-dolly\" # @param {type:\"string\"}\n",
        "DATASET_NAME = \"databricks/databricks-dolly-15k\" # @param {type:\"string\"}\n",
        "MAX_ROWS = 10 # @param {type:\"integer\"}\n",
        "UPSAMPLE = 1 # @param {type:\"integer\"}\n",
        "\n",
        "INPUT_COLUMNS = [\"instruction\", \"context\", \"response\"]\n",
        "MODEL_CONFIG = \"\"\"\n",
        "schema_version: 1.0\n",
        "models:\n",
        "  - tabllm:\n",
        "      model_id: \"gretelai/tabular-v0b\"\n",
        "      output_format: \"jsonl\"\n",
        "\"\"\"\n",
        "\n",
        "PROMPT = \"\"\"\n",
        "For each example in the Dataset, please act as a tutor and create high quality,\n",
        "detailed synthetic question and answers of higher quality than the provided example.\n",
        "Use every word from the provided 'selected_words' column in your response.\n",
        "Ensure the data teaches concepts step-by-step and focuses on improving reasoning skills.\n",
        "Focus on generating questions and answers about under-represented topics and knowledge gaps.\n",
        "\n",
        "Add two new columns to the Dataset:\n",
        "1. 'synthetic_instruction':\n",
        "  * Introduce the topic from the example briefly in 1-2 sentences\n",
        "  * Ask a clear question related to the topic that requires logical thinking or common sense reasoning\n",
        "  * Provide any necessary context to set up the reasoning problem\n",
        "  * Do not repeat the instruction from the Dataset example\n",
        "2. 'synthetic_response':\n",
        "  * Respond to the synthetically generated instruction thoroughly in a step-by-step manner\n",
        "  * Provide the complete reasoning needed to arrive at the answer\n",
        "  * Ensure the explanation is textbook quality with all details needed to learn the concept\n",
        "  * Answer in 3-5 sentences.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "JNaa7HCW0cvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper functions\n",
        "\n",
        "def setup_nltk():\n",
        "    \"\"\" Downloads necessary NLTK resources. \"\"\"\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "def display_all_data(df):\n",
        "    # Style DataFrame for better visibility and word-wrap\n",
        "    styled = df.style.set_properties(**{\n",
        "        'text-align': 'left',\n",
        "        'white-space': 'normal',\n",
        "        'height': 'auto'\n",
        "    })\n",
        "\n",
        "    # Display the styled DataFrame\n",
        "    display(styled)\n",
        "\n",
        "setup_nltk()"
      ],
      "metadata": {
        "id": "voXiMxWp00uu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def configure_gretel():\n",
        "    \"\"\" Configures the Gretel session and creates a project. \"\"\"\n",
        "    configure_session(api_key=\"prompt\", cache=\"yes\")\n",
        "    project = create_or_get_unique_project(name=PROJECT_NAME)\n",
        "    print(f\"Project URL: {project.get_console_url()}\")\n",
        "    return project\n",
        "\n",
        "def initialize_gretel_model(project):\n",
        "    \"\"\" Initialize the Gretel Tabular LLM model with the provided configuration. \"\"\"\n",
        "    model_config = yaml.safe_load(MODEL_CONFIG)\n",
        "    model = project.create_model_obj(model_config)\n",
        "    model.data_source = None\n",
        "    model.submit_cloud()\n",
        "    poll(model, verbose=False)\n",
        "    return model\n",
        "\n",
        "project = configure_gretel()\n",
        "model = initialize_gretel_model(project)"
      ],
      "metadata": {
        "id": "HBeZ9w1Z2JpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_clean_dataset(dataset_name, n_rows):\n",
        "    \"\"\" Load and clean a dataset from HuggingFace. \"\"\"\n",
        "    dataset = load_dataset(dataset_name, split='train').select(range(n_rows))\n",
        "    df = pd.DataFrame(dataset)\n",
        "    df = df.applymap(lambda x: x.replace('\\n', ' ').replace('\\r', ' ').encode('ascii', 'ignore').decode('ascii'))\n",
        "    return df\n",
        "\n",
        "df = load_and_clean_dataset(DATASET_NAME, MAX_ROWS)"
      ],
      "metadata": {
        "id": "tkDnjDHe1LYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def diversify_and_upsample(df, num_words, columns=None, new_column='selected_words', tfidf_threshold=0.2, upsample_multiplier=1):\n",
        "    \"\"\"\n",
        "    Add a new column to a DataFrame with randomly selected interesting words based on TF-IDF scores.\n",
        "    Optionally upsample the provided dataframe to create additional synthetic examples.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The input DataFrame with the new column added and upsampled rows with different selected words.\n",
        "    \"\"\"\n",
        "    if not isinstance(upsample_multiplier, int) or upsample_multiplier <= 0:\n",
        "        raise ValueError(\"Upsample multiplier must be a positive integer\")\n",
        "\n",
        "    # If columns not specified, use all columns\n",
        "    if columns is None:\n",
        "        columns = df.columns\n",
        "\n",
        "    # Combine all text data into a single string per row\n",
        "    combined_text = df[columns].apply(lambda row: ' '.join(row.astype(str)), axis=1)\n",
        "\n",
        "    # Calculate TF-IDF scores with stop words filtering\n",
        "    vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    tfidf_matrix = vectorizer.fit_transform(combined_text)\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "    # Function to process each row and select different words for each upsampled row\n",
        "    def process_row(row, row_index):\n",
        "        # Get TF-IDF scores for the current row and filter words based on the threshold\n",
        "        row_tfidf = tfidf_matrix[row_index].toarray().flatten()\n",
        "        interesting_words = [feature_names[i] for i in range(len(row_tfidf)) if row_tfidf[i] > tfidf_threshold]\n",
        "\n",
        "        # Randomly choose up to num_words without duplicates\n",
        "        sampled_words = random.sample(interesting_words, min(num_words, len(interesting_words)))\n",
        "\n",
        "        return \", \".join(sampled_words)\n",
        "\n",
        "    upsampled_rows = []\n",
        "\n",
        "    # Apply the function to each row and upsample rows with different selected words\n",
        "    for idx, row in df.iterrows():\n",
        "        sampled_words = [process_row(row, idx) for _ in range(upsample_multiplier)]\n",
        "        upsampled_rows.extend(sampled_words)\n",
        "\n",
        "    # Reset the index to match the length of upsampled rows\n",
        "    df = df.loc[df.index.repeat(upsample_multiplier)].reset_index(drop=True)\n",
        "\n",
        "    df[new_column] = upsampled_rows\n",
        "\n",
        "    return df\n",
        "\n",
        "processed_df = diversify_and_upsample(df, num_words=3, upsample_multiplier=UPSAMPLE)\n",
        "display_all_data(processed_df)"
      ],
      "metadata": {
        "id": "O3YL0g_z1C45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_synthetic_data(model, df, prompt):\n",
        "    \"\"\" Generates synthetic data using the Gretel model. \"\"\"\n",
        "    prompt_file = pathlib.Path(\"prompt.jsonl\")\n",
        "    prompt_file.write_text(json.dumps({\"prompt\": prompt}) + \"\\n\")\n",
        "\n",
        "    data_path = 'data.csv'\n",
        "    df.to_csv(data_path, index=False)\n",
        "    generator = model.create_record_handler_obj(data_source=str(prompt_file),\n",
        "                                                ref_data={\"data\": data_path},\n",
        "                                                params={\"num_records\": len(df), \"temperature\": 0.8})\n",
        "    generator.submit_cloud()\n",
        "    poll(generator, verbose=True)\n",
        "    return pd.read_json(generator.get_artifact_link(\"data\"), lines=True, compression=\"gzip\")\n",
        "\n",
        "# Create synthetic records\n",
        "synthetic = create_synthetic_data(model, processed_df, PROMPT)\n",
        "\n",
        "# Compare the example vs synthetic text-book style instructions and responses\n",
        "synthetic = synthetic[['instruction', 'response', 'synthetic_instruction', 'synthetic_response']]\n",
        "synthetic.to_csv('synthetic_data.csv', index=False)\n",
        "display_all_data(synthetic)"
      ],
      "metadata": {
        "id": "JA0y-P1P1YAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Citations\n",
        "\n",
        "@misc{li2023textbooks,\n",
        "      title={Textbooks Are All You Need II: phi-1.5 technical report},\n",
        "      author={Yuanzhi Li and S√©bastien Bubeck and Ronen Eldan and Allie Del Giorno and Suriya Gunasekar and Yin Tat Lee},\n",
        "      year={2023},\n",
        "      eprint={2309.05463},\n",
        "      archivePrefix={arXiv},\n",
        "      primaryClass={cs.CL}\n",
        "}\n",
        "\n",
        "@misc{eldan2023tinystories,\n",
        "      title={TinyStories: How Small Can Language Models Be and Still Speak Coherent English?},\n",
        "      author={Ronen Eldan and Yuanzhi Li},\n",
        "      year={2023},\n",
        "      eprint={2305.07759},\n",
        "      archivePrefix={arXiv},\n",
        "      primaryClass={cs.CL}\n",
        "}\n",
        "\n",
        "@misc{gunasekar2023textbooks,\n",
        "      title={Textbooks Are All You Need},\n",
        "      author={Suriya Gunasekar and Yi Zhang and Jyoti Aneja and Caio C√©sar Teodoro Mendes and Allie Del Giorno and Sivakanth Gopi and Mojan Javaheripi and Piero Kauffmann and Gustavo de Rosa and Olli Saarikivi and Adil Salim and Shital Shah and Harkirat Singh Behl and Xin Wang and S√©bastien Bubeck and Ronen Eldan and Adam Tauman Kalai and Yin Tat Lee and Yuanzhi Li},\n",
        "      year={2023},\n",
        "      eprint={2306.11644},\n",
        "      archivePrefix={arXiv},\n",
        "      primaryClass={cs.CL}\n",
        "}"
      ],
      "metadata": {
        "id": "DzN429DwDPqq"
      }
    }
  ]
}